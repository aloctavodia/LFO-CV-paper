---
title: "Approximate leave-future-out cross-validation for Bayesian time series models"
author: "Paul-Christian Bürkner $^{1*}$, Jonah Gabry $^2$, & Aki Vehtari $^3$"
date: |
  $^1$ Department of Psychology, University of Münster, Germany \break
  $^2$ Institute for Social and Economic Research in Policy, Columbia University, USA \break
  $^3$ Department of Computer Science, Aalto University, Finland\break
  $^*$ Corresponding author, Email: paul.buerkner@gmail.com
abstract: |
  One of the common goals of time series analysis is to use the observed series
  to inform predictions for future observations. In the absence of any actual
  new data to predict, cross-validation can be used to estimate a model's future
  predictive accuracy, for instance, for the purpose of model comparison or
  selection. As exact cross-validation for Bayesian models is often
  computationally expensive, approximate cross-validation methods have been
  developed; most notably methods for leave-one-out cross-validation (LOO-CV).
  If the actual prediction task is to predict the future given the past, LOO-CV
  provides an overly optimistic estimate as the information from future
  observations is available to influence predictions of the past. To tackle the
  prediction task properly and account for the time series structure, we can use
  leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV
  requires refitting the model many times to different subsets of the data.
  Using Pareto smoothed importance sampling, we propose a method for
  approximating exact LFO-CV that drastically reduces the computational costs
  while also providing informative diagnostics about the quality of the
  approximation.\linebreak\linebreak 
  Pareto Smoothed Importance Sampling
  Keywords: Time Series Analysis, Cross-Validation, Bayesian Inference, 
lang: en-US
class: man
# figsintext: true
numbersections: true
encoding: UTF-8
bibliography: LFO-CV
biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: true
     toc: false
header-includes:
   - \usepackage{amsmath}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{setspace}
   - \onehalfspacing
   - \setcitestyle{round}
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options: 
  chunk_output_type: console
---

```{r setup, cache = FALSE, include = FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)
options(knitr.kable.NA = '')
```

```{r packages, cache = FALSE, include = FALSE}
library(knitr)
library(kableExtra)
library(latex2exp)
library(tidyverse)
library(brms)
library(loo)
source("sim_functions.R")

# set ggplot theme
theme_set(bayesplot::theme_default())
colors <- unname(unlist(bayesplot::color_scheme_get()[c(6, 2)]))

# set rstan options
rstan::rstan_options(auto_write = TRUE)
options(mc.cores = max(1, parallel::detectCores() - 1))
```

```{r functions}
fmt <- function(x, digits = 1, ...) {
  format(x, digits = digits, nsmall = digits, ...)
}
```

# Introduction

A wide range of statistical models for time series have been developed, finding
applications in nearly all empirical sciences [e.g., see @brockwell2002;
@hamilton1994]. One common goal of a time series analysis is to use the observed
series to inform predictions for future time points. In this paper we will
assume a Bayesian approach to time series modeling, in which case if it is
possible to sample from the posterior _predictive_ distribution implied by a
given time series model, then it is straightforward to generate predictions as
far into the future as we want. When working in discrete time we will refer to
the task of predicting a sequence of $M$ future observations as $M$-step-ahead
prediction ($M$-SAP).

It is easy to evaluate the $M$-SAP performance of a time series
model by comparing the predictions to the observed sequence of $M$ future data
points once they become available. However, we would often like to estimate 
the future predictive performance of a model _before_ we are able to collect 
additional observations. If there are many competing models we may also need to
first decide which model (or which combination of the models) to
rely on for prediction [@geisser1979; @hoeting1999; @vehtari2002; @ando2010; 
@vehtari2012]. 

In the absence of new data with which to evaluate predictive performance, one
general approach for evaluating a model's predictive accuracy is
cross-validation. The data is first split into two subsets, then we fit the
statistical model to the first subset and evaluate predictive performance with
the second subset. We may do this once or many times, each time leaving out a
different subset.

If the data points are not ordered in time, or if the goal is to assess the
non-time-dependent part of the model, then we can use leave-one-out
cross-validation (LOO-CV). For a data set with $N$ observations, we refit the
model $N$ times, each time leaving out one of the $N$ observations and assessing
how well the model predicts the left-out observation. Due to the number of
required refits, exact LOO-CV is computationally expensive, in particular when
performing full Bayesian inference and refitting the model means estimating a
new posterior distribution rather than a point estimate. But it is it is
possible to approximate exact LOO-CV using Pareto smoothed importance sampling
[PSIS; @vehtari2017loo; @vehtari2017psis]. PSIS-LOO-CV only requires a single
fit of the full model and has sensitive diagnostics for assessing the validity
of the approximation.

However, LOO-CV is problematic for times series models if the goal is to
estimate the predictive performance for future time points. In that case,
leaving out only one observation at a time will allow information from the
future to influence predictions of the past (i.e., times $t+1, t+2, \ldots$
would be used to predict time $t$). Instead, to apply the idea of
cross-validation to the $M$-SAP case we can use what we will refer to as
leave-*future*-out cross-validation (LFO-CV). LFO-CV does not refer to one
particular prediction task but rather to various possible cross-validation
approaches that all involve some form of prediction of future time points. Like
exact LOO-CV, exact LFO-CV requires refitting the model many times to different
subsets of the data, which is computationally expensive, in particular when
performing full Bayesian inference.

In this paper, we extend the ideas from PSIS-LOO-CV and present PSIS-LFO-CV, an
algorithm that typically only requires refitting a time-series model a small
number times. This will make LFO-CV tractable for many more realistic
applications than previously possible, including time series model averaging
using stacking of predictive distributions [@yao2018].

The structure of the paper is as follows. In Section \@ref(m-sap), we introduce
the idea and various forms of $M$-step-ahead predictions and how to approximate
them using PSIS. In Section \@ref(simulations), we evaluate the accuracy of the
approximation using extensive simulations. Then, in Section \@ref(case-studies),
we provide two real world case studies. One analyzing the change in level of
Lake Huron and the other examining when the annual day of the cherry blossoms in
Kyoto, Japan occurred, with the timeline starting in the 9th century. We end in
Section \@ref(discussion) with a discussion of the usefulness and limitations of
our approach.

# $M$-step-ahead predictions {#m-sap}

Assume we have a time series of observations $y = (y_1, y_2, \ldots, y_N)$ 
and let $L$ be the _minimum_ number of observations from the series that
we will require before making predictions for future data. Depending on the
application and how informative the data are, it may not be possible to make
reasonable predictions for $y_{i+1}$ based on $(y_1, \dots, y_{i})$ until $i$ is
large enough so that we can learn enough about the time series to predict future
observations. Setting $L=10$, for example, means that we will only assess
predictive performance starting with observation $y_{11}$, so that we
always have at least 10 previous observations to condition on.

In order to assess $M$-SAP performance we would like to compute the 
predictive densities

\begin{equation}
p(y_{i+1:M} \,|\, y_{1:i}) = 
  p(y_{i+1}, \ldots, y_{i + M} \,|\, y_{1},...,y_{i}) 
\end{equation}

for each $i \in \{L, \ldots, N - M\}$, where we use 
$y_{i+1:M} = (y_{i+1}, \ldots, y_{i + M})$ and $y_{1:i} = (y_{1}, \ldots, y_{i})$ 
to shorten the notation. As a global measure of predictive accuracy, we
can use the expected log posterior density [ELPD; @vehtari2017loo], which, 
for M-SAP, can be defined as

\begin{equation}
\label{ELPD}
{\rm ELPD} = \sum_{i=L}^{N - M} 
  \int p_t(\tilde{y}_{i+1:M}) \log p(\tilde{y}_{i+1:M} \,|\, y_{1:i})
  \, {\rm d} \, \tilde{y}_{i+1:M}.
\end{equation}

The distribution $p_t(\tilde{y}_{i+1:M})$ describes the true data generating
process for new data $\tilde{y}_{i+1:M}$. As these true data generating processes
are unknown, we approximate the ELPD using LFO-CV, which leads to
 
\begin{equation}
{\rm ELPD}_{\rm LFO} = \sum_{i=L}^{N - M} \log p(y_{i+1:M} \,|\, y_{1:i}).
\end{equation}

The quantities $p(y_{i+1:M} \,|\, y_{1:i})$ can be computed with the help of the
posterior distribution $p(\theta \,|\, y_{1:i})$ of the parameters $\theta$
conditional on only the first $i-1$ observations of the time-series:

\begin{equation}
\label{Lpred}
p(y_{i+1:M} \,| \, y_{1:i}) = 
  \int p(y_{i+1:M} \,| \, y_{1:i}, \theta) \, 
    p(\theta\,|\,y_{1:i}) \, {\rm d} \theta. 
\end{equation}

For factorizable models, the response values are conditionally 
independent given the parameters, and the likelihood can be written 
in the factorized form

\begin{equation}
p(y \,|\, \theta) = \prod_{n=1}^N p(y_n \,|\, \theta).
\end{equation}

In this case, $p(y_{i+1:M} \,|\, y_{1:i}, \theta)$ reduces to 
\begin{equation}
p(y_{i+1:M} \,|\, y_{1:i}, \theta) =p(y_{i+1:M} \,|\, \theta) = 
\prod_{n = i+1}^{i + M} p(y_n \,|\, \theta),
\end{equation}
due to the assumption of conditional independence between $y_{i+1:M}$ and $y_{1:i}$
given $\theta$. Cross-validation for non-factorizable models, which 
does not make this assumption, is discussed in @buerkner:non-factorizable.

In practice, we will not be able to directly solve the integral in 
(\@ref(Lpred)), but instead have to use Monte-Carlo methods to approximate it.
Having obtained $S$ random draws $(\theta_{1:i}^{(1)}, \ldots, \theta_{1:i}^{(S)})$ 
from the posterior distribution $p(\theta\,|\,y_{1:i})$, we can estimate 
$p(y_{i+1:M} | y_{1:i})$ as

\begin{equation}
p(y_{i+1:M} \,|\, y_{1:i}) \approx \frac{1}{S}
\sum_{s=1}^S p(y_{i+1:M} \,|\, y_{1:i}, \theta_{1:i}^{(s)}),
\end{equation}

which further simplifies for factorizable models as shown above.

## Approximate $M$-step-ahead predictions {#approximate-MSAP}

The above equations include the posterior distributions from many
different fits of the model to different subsets of the data. To obtain
the predictive density $p(y_{i+1:M} \,|\, y_{1:i})$, a model is fit to
only the first $i-1$ data points, and we will need to do this for every value of
$i$ under consideration (i.e., all $i \in \{L + 1, \ldots, N - M + 1\}$).
Below, we will present a new algorithm to reduce the number of models that need
to be fit for the purpose of obtaining each of the densities $p(y_{i+1:M} \,|\,
y_{1:i})$. This algorithm relies in a central manner on Pareto smoothed
importance sampling [@vehtari2017loo; @vehtari2017psis], which we will briefly
review next.

### Pareto smoothed importance sampling {#psis}

Importance sampling is a technique for compute expectations with
respect to some target distribution using an approximating proposal distribution
that is easier to draw samples from than the actual target. If $f(\theta)$ is
the target and $g(\theta)$ is the proposal distribution, we can write any
expectation of some function $h(\theta)$ with respect to $f$ as

\begin{equation}
\mathbb{E}_f[h(\theta)] = \int h(\theta) f(\theta) \,d\, \theta 
 = \frac{\int [h(\theta) f(\theta) / g(\theta)] g(\theta) \,d\, \theta}
    {\int [f(\theta) / g(\theta)] g(\theta) \,d\, \theta} 
 = \frac{\int h(\theta) r(\theta) g(\theta) \,d\, \theta}
    {\int r(\theta) g(\theta) \,d\, \theta}
\end{equation}

with importance ratios 
\begin{equation}
r(\theta) = \frac{f(\theta)}{g(\theta)}.
\end{equation}

Accordingly, if $\theta^{(s)}$ are $S$ random draws from $g(\theta)$, we can
approximate

\begin{equation}
\mathbb{E}_f[h(\theta)] \approx 
\frac{\sum_{s=1}^S h(\theta^{(s)}) r(\theta^{(s)})}{\sum_{s=1}^S r(\theta^{(s)})},
\end{equation}

provided that we can compute the raw importance ratios $r(\theta^{(s)})$ up to
some multiplicative constant. The raw importance ratios serve as 
weights on the corresponding random draws in the approximation of the quantity 
of interest. The main problem with this approach is that
the raw importance ratios tend to have large or infinite variance and
results can be highly unstable. 

In order to stabilize those computations, one solution is to regularize the
largest raw importance ratios using the corresponding quantiles of generalized
Pareto distribution fitted to the largest raw importance ratios. This procedure
is called Pareto smooth importance sampling [PSIS; @vehtari2017loo;
@vehtari2017psis] and has been demonstrated to have a lower error and faster
convergence rate than other commonly used regularization techniques
[@vehtari2017psis]. In addition, PSIS comes with a useful diagnostic to evaluate
the quality of the importance sampling approximation. The shape parameter $k$
of the generalized Pareto distribution fit to the largest importance ratios provides
information about the number of existing moments of the weight distribution and
the actual importance sampling estimate. When $k<0.5$, the weight distribution
has finite variance, and as a result of the central limit theorem, the convergence of
the importance sampling estimate with increasing number of draws will be fast. 
This implies that approximate LOO-CV via PSIS is highly accurate for $k<0.5$ 
[@vehtari2017psis]. For $0.5 \leq k < 1$, a generalized central limit
theorem holds, but the convergence rate drops quickly as $k$ increases
[@vehtari2017psis].  In practice, PSIS has been shown to be relatively robust
for $k < 0.7$ [@vehtari2017loo; @vehtari2017psis]. As such, the default 
threshold is set to $0.7$ when performing PSIS LOO-CV [@vehtari2017loo;@loo2018].


### PSIS applied to $M$-step-ahead predictions

We now turn back to our task of performing $M$-step-ahead predictions for
time-series models. Starting with $i = N - M$, we approximate each
$p(y_{i+1:M} \,|\, y_{1:i})$ via

\begin{equation}
 p(y_{i+1:M} \,|\, y_{1:i}) \approx
   \frac{ \sum_{s=1}^S w_i^{(s)}\, p(y_{i+1:M} \,|\, \theta^{(s)})}
        { \sum_{s=1}^S w_i^{(s)}},
\end{equation}

where $w_i^{(s)}$ are the PSIS weights and $\theta^{(s)}$ are draws from the
posterior distribution based on _all_ observations. To obtain $w_i^{(s)}$, we
first compute the raw importance ratios

\begin{equation}
r_i^{(s)} = r_i(\theta^{(s)}) = \frac{f_i(\theta^{(s)})}{g(\theta^{(s)})} 
\propto \frac{
\prod_{j \in J \backslash J_i} p(y_j \,|\, \,\theta^{(s)}) p(\theta^{(s)})
}{
\prod_{j \in J} p(y_j \,|\, \,\theta^{(s)}) p(\theta^{(s)})
}
= \frac{1}{\prod_{j \in J_i} p(y_j \,|\, \,\theta^{(s)})},
\end{equation}

with $J = \{1, \ldots, N\}$, and then stabilize them using PSIS as described
above. The index set $J_i$ contains the indices of all observations which are
part of the data for the model being fitted but not for the model whose
predictive performance we are trying to approximate. That is, for the starting
value $i = N - M$, we have $J_i = \{i+1, \ldots, N\}$. This approach to
computing importance ratios is a generalization of the approach used in
PSIS-LOO-CV, where only a single observation is left out at a time and thus 
$J_i = \{i\}$ for all $i$.

Starting from $i = N - M$, we gradually *decrease* $i$ by $1$ (i.e., we move
backwards in time) and repeat the process. At some observation $i$, the
variability of the importance ratios $r_i^{(s)}$ will become too large and
importance sampling fails. We will refer to this particular value of $i$ as
$i^\star_1$. To identify the value of $i^\star_1$, we check for which value of
$i$ does the estimated shape parameter $k$ of the generalized Pareto
distribution first cross a certain threshold $\tau$ [@vehtari2017psis]. Only
then do we refit the model using only observations up to $i^\star_1$ and then
restart the process. Until the next refit, we thus have $J_i = \{i+1, \ldots,
i^\star_1 \}$ for $i < i^\star_1$, as the refitted model only contains the
observations up to index $i^\star_1$. An illustration of this procedure is shown
in Figure \@ref(fig:vis-msap).

In some cases we may only need to refit once and in other cases we will find a
value $i^\star_2$ that requires a second refitting, maybe an $i^\star_3$ that
requires a third refitting, and so on. We repeat the refitting as many times as
is required (only if $k > \tau$) until we arrive at $i = L$. Recall that $L$
is the minimum number of observations we have deemed acceptable for making
predictions (setting $L=0$ means the first data point will be predicted only
based on the prior). A detailed description of the algorithm in the form of
pseudo code is provided in Appendix A. If the data contains multiple independent
time-series, the algorithm should be applied to each of the time-series
separately, and the resulting ELPD values can be summed up afterwards.

```{r vis-msap, fig.width=8, fig.height=3, fig.cap="Visualisation of PSIS approximated one-step-ahead predictions leaving out all future values. Predicted observations are indicated by **X**. In the shown example, the model was last refit at the $i^\\star = 4$th observation."}
status_levels <- c("included", "left out", "left out (PSIS)")
df <- data.frame(
  obs = rep(1:9, 3),
  i = factor(rep(3:5, each = 9)),
  Status = c(
    rep("included", 2), rep("left out (PSIS)", 2), rep("left out", 5),
    rep("included", 3), rep("left out (PSIS)", 1), rep("left out", 5),
    rep("included", 4), rep("left out", 5)
  )
) %>%
  mutate(Status = factor(Status, levels = status_levels))

msap_colors <- c(
  bayesplot::color_scheme_get("viridis")$light,
  bayesplot::color_scheme_get("viridis")$dark,
  bayesplot::color_scheme_get("viridis")$mid_highlight
)

ggplot(df, aes(obs, i, fill = Status)) +
  geom_tile(height = 0.9, width = 1, col = "black") +
  annotate(
    'text', x = 3:5, y = c(1, 2, 3), 
    label = "X", parse = TRUE, 
    size = 10, color = "white"
  ) +
  labs(x = "Observation", y = "Predicted observation") +
  scale_x_continuous(breaks = 1:9) +
  scale_fill_manual(values = msap_colors) +
  bayesplot::theme_default() +
  NULL
```

The threshold $\tau$ is crucial to the accuracy and speed of the algorithm. If
$\tau$ is too large then we need fewer refits and thus achieve higher speed, but
accuracy is likely to suffer. If $\tau$ is too small, the accuray will be high
but many refits will be required and overall speed will drop noticeably. When
performing exact cross-validation of Bayesian models, almost all of the
computational time is spent fitting models, while the time needed to compute
predictions is negligible in comparison. That is, a reduction in the number of
refits essentially implies a proportional reduction in the overall time required
for cross-validation of Bayesian models.

For the PSIS-LFO-CV algorithm introduced in this paper, we can expect an
appropriate threshold to be somewhere between $0.5 \leq \tau \leq 0.7$. It is
unlikely to be as high as the $\tau = 0.7$ default used for PSIS-LOO-CV because
there will be more dependence in the errors when doing PSIS-LFO-CV. If there is
a large error when leaving out the $i$th observation, then there is likely to
also be a large error when leaving out observations $i, i-1, i-2, \ldots$ until
a refit is performed. That is, highly influential observations corresponding to
a large $k$ estimate are likely to have stronger effects on the total estimate
for LFO-CV than for LOO-CV. We will come back to the issue of setting
appropriate thresholds in Section \@ref(simulations).

# Simulations {#simulations}

To evaluate the quality of the PSIS-LFO-CV approximation, we performed a
simulation study. The following conditions were systematically varied: 

* The number $M$ of future observations to be predicted took on values of $M =
1$ and $M = 4$.
* The threshold $\tau$ of the Pareto $k$ estimates was varied between $k = 0.5$
to $k = 0.7$ in steps of $0.1$.
* Six different data generating models were evaluated, with linear and/or
quadratic terms and/or autoregressive terms of order 2 (see Figure 
\@ref(fig:simmodels)).

In all cases the time-series consisted of $N = 200$ observations and the minimal
number of observations required before make predictions was set to $L = 25$.

```{r, include = FALSE}
seed <- 1234
set.seed(seed)
N <- 200
time <- seq_len(N)
stime <- scale_unit_interval(time)
models <- c(
  "constant", "linear", "quadratic",
  "AR2-only", "AR2-linear", "AR2-quadratic"
)

fits <- preds <- setNames(vector("list", length(models)), models)
for (m in names(fits)) {
   file <- paste0("models/fit_", m)
   fits[[m]] <- fit_model(model = m, N = N, seed = seed, file = file)
   pred <- posterior_predict(fits[[m]])
   preds[[m]] <- fits[[m]]$data %>% 
     mutate(       
       time = time, 
       stime = stime,
       Estimate = colMeans(pred), 
       Q5 = apply(pred, 2, quantile, probs = 0.05),
       Q95 = apply(pred, 2, quantile, probs = 0.95),
       model = m
    )
}
preds <- as_tibble(bind_rows(preds)) %>%
   mutate(model = factor(model, levels = models)) %>%
   select(-`I(stime^2)`)
```

```{r simmodels, fig.height=4, fig.cap="Illustration of the models used in the simulations."}
ggplot(preds, aes(x = time, y = Estimate)) +
  facet_wrap(~model) +
  geom_smooth(aes(ymin = Q5, ymax = Q95), stat = "identity", size = 0.5) +
  geom_point(aes(y = y), size = 0.5) +
  labs(y = "y")
```

Autoregressive (AR) models are some of the most commonly used time-series models. 
An AR(p) model -- an autoregressive model of order $p$ -- can be defined as

\begin{equation}
y_i = \eta_i + \sum_{k = 1}^p \varphi_k y_{i - k} + \varepsilon_i,
\end{equation}

where $\eta_i$ is the linear predictor for the $i$th observation, $\varphi_k$
are the autoregressive parameters and $\varepsilon_i$ are pairwise independent
errors, which are usually assumed to be normally distributed with equal variance
$\sigma^2$. The model implies a recursive formula that allows for computing the
right-hand side of the equation for observation $i$ based on the values of the
equations computed for previous observations. Thus, by definition, responses of
AR-models are not conditionally independent. However, they are still
factorizable because we can write down a separate likelihood contribution per
observation [see @buerkner:non-factorizable for more discussion on
factorizability of statistical models].

In addition to exact and approximate LFO-CV, we also compute approximate LOO-CV
for comparison. This is not because we think LOO-CV is a generally appropriate
approach for time-series models, but because, in the absence of any approximate
LFO-CV method, researchers may have used approximate LOO-CV for time-series
models in the past simply because it was available. As such, demonstrating that
LOO-CV is a biased estimate of LFO-CV underlines the importance of developing
methods better suited for the task.

All simulations were done in R [@R2018] using the brms package [@brms1;
@brms2] together with the probabilistic programming language Stan
[@carpenter2017] for the modeling fitting, the loo package [@loo2018] for
the PSIS computation, and several tidyverse packages [@tidyverse] for data
processing. The full code and all results are available on Github (https://github.com/paul-buerkner/LFO-CV-paper).

## Results {#sim_results}

```{r}
mlevels <- c(
  "constant", "linear", "quadratic",
  "AR2-only", "AR2-linear", "AR2-quadratic"
)
tau_levels <- TeX(paste0("$\\tau$ = ", c(0.5, 0.6, 0.7)))
lfo_sims <- read_rds("results/lfo_sims.rds") %>%
  as_tibble() %>%
  mutate(
    model = factor(model, levels = mlevels),
    tau = factor(k_thres, labels = tau_levels),
    elpd_loo = map_dbl(res, ~ .$loo_cv$estimates["elpd_loo", 1]),
    elpd_exact_lfo = map_dbl(res, ~ .$lfo_exact_elpd[1]),
    elpd_approx_lfo = map_dbl(res, ~ .$lfo_approx_elpd[1]),
    elpd_diff_lfo = elpd_approx_lfo - elpd_exact_lfo,
    elpd_diff_loo = elpd_loo - elpd_exact_lfo,
    npreds = map_dbl(res, ~ sum(!is.na(.$lfo_approx_elpds))),
    nrefits = lengths(map(res, ~ attr(.$lfo_approx_elpds, "refits"))),
    rel_nrefits = nrefits / npreds
  )
```

Results of the 1-SAP simulations are visualized
in Figure \@ref(fig:1sap). Comparing the columns of Figure \@ref(fig:1sap), it
is clearly visible that the accuracy of the PSIS approximation increases with
decreasing $\tau$, up to almost perfect accuracy for $\tau = 0.5$. At the same 
time, the proportion of observations at which refitting the model was required
increased substantially with decreasing $\tau$ (see Table 
\@ref(tab:refits)). Using $\tau = 0.6$ induced a slight positive
bias in PSIS-LFO-CV, but also reduced the number of required refits by roughly 
$30\%$. Another $30\%$ reduction in the number of refits was achieved by
using $\tau = 0.7$ but at the cost of disproportionally increasing the 
positive bias in PSIS-LFO-CV. As expected, LOO-CV is a biased estimate of the 
1-SAP performance for all non-constant models in particular those with a trend 
in the time-series (see the lighter histograms in Figure \@ref(fig:1sap)).

```{r 1sap, fig.height=8, fig.cap="Simulation results of 1-step-ahead predictions. Histograms are based on 100 simulation trials of time-series with $N = 200$ observations requiring at least $L = 25$ observations to make predictions."}
lfo_sims %>% 
  filter(is.na(B), M == 1) %>% 
  select(elpd_diff_lfo, elpd_diff_loo, model, tau) %>%
  gather("Type", "elpd_diff", elpd_diff_lfo, elpd_diff_loo) %>%
  ggplot(aes(x = elpd_diff, y = ..density.., fill = Type)) +
  facet_grid(
    model ~ tau, scales = "free_y", 
    labeller = label_parsed
  ) +
  geom_histogram(alpha = 0.7) +
  scale_fill_manual(
    values = colors,
    labels = c("Approximate LFO-CV", "Approximate LOO-CV")
  ) +
  labs(x = 'ELPD difference to exact LFO-CV', y = "Density") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  NULL
```

```{r refits, cache=FALSE}
lfo_sims %>% 
  filter(is.na(B)) %>%
  select(model, M, k_thres, rel_nrefits) %>%
  group_by(model, M, k_thres) %>%
  summarise(rel_nrefits = round(mean(rel_nrefits), 2)) %>%
  ungroup() %>%
  spread("model", "rel_nrefits") %>%
  mutate(M = ifelse(duplicated(M), "", M)) %>%
  rename(`$\\tau$` = "k_thres") %>%
  kable(
    caption = "Mean proportions of required refits.",
    booktabs = TRUE,
    escape = FALSE
  ) %>%
  footnote(
    general = "Note: Results are based on 100 simulation trials of time-series with $N = 200$ observations requiring at least $L = 25$ observations to make predictions. Abbreviations: $\\\\tau$ = threshold of the Pareto $k$ estimates; $M$ = number of predicted future observations.",
    general_title = "",
    threeparttable = TRUE,
    escape = FALSE
  )
```

Results of the 4-SAP simulations are visualized in
Figure \@ref(fig:4sap). Comparing the columns of Figure \@ref(fig:4sap), it is
clearly visible that the accuracy of the PSIS approximation increases with
decreasing $\tau$, up to almost perfect accuracy for $\tau = 0.5$. At the same
time, the proportion of observations at which refitting the model was required
increased substantially with decreasing $\tau$ (see Table \@ref(tab:refits)). In
light of the corresponding 1-SAP results presented above, this is not surprising as
the procedure for determining the necessity of a refit is independent of $M$ (see
Section \@ref(approximate-MSAP)). Using $\tau = 0.6$ again induced a slight
positive bias in PSIS-LFO-CV, but also reduced the number of required refits by
roughly $30\%$. Another $30\%$ reduction in the number of refits was achieved by
using $\tau = 0.7$ but at the cost of disproportionally increasing the positive
bias in PSIS-LFO-CV. PSIS-LOO-CV is not displayed in Figure \@ref(fig:4sap) as
the number of observations predicted as each step (4 vs. 1) renders 4-SAP LFO-CV
and LOO-CV incomparable.

```{r 4sap, fig.height=8, fig.cap="Simulation results of 4-step-ahead predictions. Histograms are based on 100 simulation trials of time-series with $N = 200$ observations requiring at least $L = 25$ observations to make predictions."}
lfo_sims %>% 
  filter(is.na(B), M == 4) %>%
  select(elpd_diff_lfo, model, tau) %>%
  ggplot(aes(x = elpd_diff_lfo, y = ..density..)) +
  facet_grid(
    model ~ tau, scales = "free_y", 
    labeller = label_parsed
  ) +
  geom_histogram(alpha = 0.7, fill = colors[1]) +
  labs(x = 'ELPD difference of approximate and exact LFO-CV', y = "Density") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  NULL
```

# Case Studies {#case-studies}

## Annual measurements of the level of Lake Huron {#case-LH}

To illustrate the application of PSIS-LFO-CV for estimating expected $M$-SAP
performance, we will fit a model for 98 annual measurements of the water level
(in feet) of [Lake Huron](https://en.wikipedia.org/wiki/Lake_Huron) from the
years 1875--1972. This data set is found in the *datasets* R package, which is
installed automatically with R [@R2018]. The time-series shows rather strong
autocorrelation and some downward trend towards lower water levels for later points
in time. Figure \@ref(fig:lake-huron) shows the observed time series of water 
levels as well as predictions from a fitted AR(4) model.

```{r}
data("LakeHuron")
N <- length(LakeHuron)
df <- data.frame(
  y = as.numeric(LakeHuron),
  year = as.numeric(time(LakeHuron)),
  time = 1:N
) 
```

```{r fit_lh, results = "hide"}
fit_lh <- brm(
  y | mi() ~ 1, 
  data = df, 
  autocor = cor_ar(~time, p = 4), 
  prior = prior(normal(0, 0.5), class = "ar"),
  chains = 2, warmup = 1000, iter = 5000,
  control = list(adapt_delta = 0.99),
  seed = 5838296, file = "models/fit_lh"
)
```

```{r lake-huron, fig.cap="Water Level in Lake Huron (1875-1972). Black points are observed data. The blue line represents mean predictions of an AR(4) model with 90% prediction intervals shown in gray.", fig.height=3}
preds <- posterior_predict(fit_lh)
preds <- cbind(
  Estimate = colMeans(preds), 
  Q5 = apply(preds, 2, quantile, probs = 0.05),
  Q95 = apply(preds, 2, quantile, probs = 0.95)
)

ggplot(cbind(df, preds), aes(x = year, y = Estimate)) +
  geom_smooth(aes(ymin = Q5, ymax = Q95), stat = "identity", size = 0.5) +
  geom_point(aes(y = y)) + 
  labs(y = "Water Level (ft)", x = "Year")
```

```{r}
L <- 20
k_thres <- 0.6
loo_lh <- loo(log_lik(fit_lh)[, (L + 1):N])
```

Based on this data and model, we will illustrate the use of PSIS-LFO-CV to
provide estimates of $1$-SAP and $4$-SAP when leaving out all future values. To allow
for reasonable predictions, we will require at least $L = 20$
historical observations (20 years) to make predictions. Further, we set a
threshold of $\tau =$ `r k_thres` for the Pareto $k$ estimates that indicate when
refitting becomes necessary. Our fully reproducible analysis of this case
study can be found on GitHub (https://github.com/paul-buerkner/LFO-CV-paper).

```{r}
M <- 1
lh_elpd_1sap_exact <- compute_lfo(
  fit_lh, type = "exact", M = M, L = L, 
  file = "results/lh_elpd_1sap_exact.rds"
)
lh_elpd_1sap_approx <- compute_lfo(
  fit_lh, type = "approx", M = M, L = L, 
  file = "results/lh_elpd_1sap_approx.rds"
)
refits <- attributes(lh_elpd_1sap_approx)$refits
nrefits <- length(refits)

sum_lh_elpd_1sap_exact <- summarize_elpds(lh_elpd_1sap_exact)[1]
sum_lh_elpd_1sap_approx <- summarize_elpds(lh_elpd_1sap_approx)[1]
```

```{r}
M <- 4
lh_elpd_4sap_exact <- compute_lfo(
  fit_lh, type = "exact", M = M, L = L, 
  file = "results/lh_elpd_4sap_exact.rds"
)
lh_elpd_4sap_approx <- compute_lfo(
  fit_lh, type = "approx", M = M, L = L, 
  file = "results/lh_elpd_4sap_approx.rds"
)
sum_lh_elpd_4sap_exact <- summarize_elpds(lh_elpd_4sap_exact)[1]
sum_lh_elpd_4sap_approx <- summarize_elpds(lh_elpd_4sap_approx)[1]
```

We start by computing exact and PSIS-approximated LFO-CV of 1-SAP. The computed
ELPD values are
${\rm ELPD}_{\rm exact} =$ `r fmt(sum_lh_elpd_1sap_exact, 2)` and 
${\rm ELPD}_{\rm approx} =$ `r fmt(sum_lh_elpd_1sap_approx, 2)`, which are
almost identical. Not only is the overall ELPD estimated accurately but so are
all of the pointwise ELPD contributions (see the left panel of Figure
\@ref(fig:lh-pw-elpd)). In comparison, PSIS-LOO-CV returns
${\rm ELPD}_{\rm loo} =$ `r fmt(loo_lh$estimates[1, 1], 1)`,
overestimating the predictive performance and as suggested by our simulation
results for stationary autoregessive models (see fourth row of Figure
\@ref(fig:1sap)). Plotting the Pareto $k$ estimates reveals that the model had
to be refit `r nrefits` times, out of a total of $N - L =$ `r N - L` predicted
observations (see Figure \@ref(fig:lh-pareto-k)). On average, this means one
refit every `r fmt((N - L) / nrefits, 1)` observations, which implies a drastic
speed increase compared to exact LFO-CV.

Performing LFO-CV for 4-SAP, we obtained ${\rm ELPD}_{\rm exact} =$ 
`r fmt(sum_lh_elpd_4sap_exact, 2)` and ${\rm ELPD}_{\rm approx} =$ 
`r fmt(sum_lh_elpd_4sap_approx, 2)`, which are again almost identical.
In general, as $M$ increases, the approximation will tend to become more variable
around the true value in absolute ELPD units because the ELPD increment of each 
observation will be based on more and more observations (see also Section 
\@ref(simulations)). For this example, we see some
differences in the pointwise ELPD contributions of specific observations which
were hard to predict accurately by the model (see the right panel of Figure
\@ref(fig:lh-pw-elpd)). However, these differences cancel out in
the overall ELPD estimate. Since, for constant threshold $\tau$, the importance
weights are the same independent of $M$, the Pareto $k$ estimates are also the 
same in $4$-SAP as in $1$-SAP.

```{r lh-pw-elpd, warning=FALSE, fig.height=3, fig.cap="Pointwise exact vs. PSIS-approximated ELPD contributions for 1-SAP (left) and 4-SAP (right) for the Lake Huron model. A threshold of $\\tau = 0.6$ was used for the Pareto $k$ estimates. $M$ is the number of predicted future observations."}
lh_pw_elpd <- tibble(
  elpd_exact = na.omit(lh_elpd_1sap_exact),
  elpd_approx = na.omit(lh_elpd_1sap_approx),
  k = na.omit(attributes(lh_elpd_1sap_approx)$ks),
  M = "M = 1"
) %>% bind_rows(
  tibble(
    elpd_exact = na.omit(lh_elpd_4sap_exact),
    elpd_approx = na.omit(lh_elpd_4sap_approx),
    k = na.omit(attributes(lh_elpd_4sap_approx)$ks),
    M = "M = 4"
  )
)
ggplot(lh_pw_elpd, aes(elpd_exact, elpd_approx)) +
  facet_wrap(facets = "M", nrow = 1, ncol = 2, scales = "free") +
  geom_abline(slope = 1) +
  geom_point() +
  labs(y = "Approximate ELPD", x = "Exact ELPD") + 
  theme_bw()
```

```{r lh-pareto-k, fig.height=2.5, fig.cap="Pareto $k$ estimates for PSIS-LFO-CV of the Lake Huron model leaving out all future values. The dotted red line indicates the threshold at which the refitting was necessary."}
ks <- na.omit(attributes(lh_elpd_1sap_approx)$ks)
ids <- (L + 1):N
plot_ks(ks, ids)
```


## Annual date of the cherry blossoms in Japan {#case-CB}

```{r}
cherry <- read.csv("data/cherry_blossoms.csv")
cherry_temp <- cherry[!is.na(cherry$temp), ]
cherry_doy <- cherry[!is.na(cherry$doy), ]
```

The cherry blossom in Japan is a famous natural phenomenon occurring once every
year during spring. As climate changes so does the annual date of the cherry
blossom [@aono2008; @aono2010]. The most complete reconstruction available to
date contains data between `r min(cherry$year)` AD and `r max(cherry$year)` AD
[@aono2008; @aono2010]. The data is freely available online
(http://atmenv.envi.osakafu-u.ac.jp/aono/kyophenotemp4/).

In this case study, we are going to predict the annual date of the cherry
blossom using an approximate Gaussian process model [@solin2014, @RiutortMayol2019] to
provide flexible non-linear smoothing of the time-series. A visualisation of
both the data and the fitted model in provided in Figure
\@ref(fig:cherry-blossom). While the time-series appears rather stable across
earlier centuries, with substantial variation across consecutive years, there
are some clearly visible trends in the data. Particularly in more recent years,
the cherry blossom has tended to happen much earlier than before, which may 
be a consequence of changes in the climate [@aono2008; @aono2010].

Based on this data and model, we will illustrate the use of PSIS-LFO-CV to
provide estimates of $1$-SAP and $4$-SAP leaving out all future values. To allow
for reasonable predictions of future values, we will require at least $L = 100$
historical observations (100 years) to make predictions. Further, we set a
threshold of $\tau =$ `r k_thres` for the Pareto $k$ estimates to determine when
refitting becomes necessary. Our fully reproducible analysis of this case study 
can be found on GitHub (https://github.com/paul-buerkner/LFO-CV-paper).

```{r fit_cb}
fit_cb <- brm(
  formula = bf(doy ~ gp(year, k = 20, c = 5/4)), 
  data = cherry_doy, 
  prior = prior(normal(0, 0.1), class = lscale, coef = gpyear),
  chain = 2, warmup = 4000, iter = 7000, inits = 0,
  control = list(adapt_delta = 0.99, max_treedepth = 15),
  seed = 5838234, file = "models/fit_cb"
)
```

```{r cherry-blossom, fig.height=3, fig.cap="Day of the cherry blossom in Japan (812-2015). Black points are observed data. The blue line represents mean predictions of a thin-plate spline model with 90% regression intervals shown in gray."}
me_cb <- marginal_effects(fit_cb, probs = c(0.05, 0.95))
plot(me_cb, points = TRUE, plot = FALSE)[[1]] +
  labs(x = "Year", y = "Day of cherry blossom")
```

```{r}
N <- NROW(cherry_doy)
L <- 100
M <- 1
cb_elpd_1sap_exact <- compute_lfo(
  fit_cb, type = "exact", M = M, L = L, 
  file = "results/cb_elpd_1sap_exact.rds"
)
cb_elpd_1sap_approx <- compute_lfo(
  fit_cb, type = "approx", M = M, L = L, 
  file = "results/cb_elpd_1sap_approx.rds"
)
refits <- attributes(cb_elpd_1sap_approx)$refits
nrefits <- length(refits)

sum_cb_elpd_1sap_exact <- summarize_elpds(cb_elpd_1sap_exact)[1]
sum_cb_elpd_1sap_approx <- summarize_elpds(cb_elpd_1sap_approx)[1]

loo_cb <- loo(fit_cb, newdata = cherry_doy[-seq_len(L), ])
```

```{r}
M <- 4
cb_elpd_4sap_exact <- compute_lfo(
  fit_cb, type = "exact", M = M, L = L, 
  file = "results/cb_elpd_4sap_exact.rds"
)
cb_elpd_4sap_approx <- compute_lfo(
  fit_cb, type = "approx", M = M, L = L, 
  file = "results/cb_elpd_4sap_approx.rds"
)

sum_cb_elpd_4sap_exact <- summarize_elpds(cb_elpd_4sap_exact)[1]
sum_cb_elpd_4sap_approx <- summarize_elpds(cb_elpd_4sap_approx)[1]
```

We start by computing exact and PSIS-approximated LFO-CV of 1-SAP. We compute
${\rm ELPD}_{\rm exact} =$ `r fmt(sum_cb_elpd_1sap_exact, 1)` and 
${\rm ELPD}_{\rm approx} =$ `r fmt(sum_cb_elpd_1sap_approx, 1)`, which are highly
similar. PSIS-LFO-CV slightly overestimates the predictive performance for 
$\tau =$ `r k_thres`, which is in line with our simulation results (see Section
\@ref(simulations)). However, as the difference is so small, it may also just be
random error. As shown in the left panel of Figure \@ref(fig:cb-pw-elpd), the
pointwise ELPD contributions are highly accurate, with no outliers, indicating
the our approximation has worked well consistently across observations.
PSIS-LFO-CV performs much better than PSIS-LOO-CV (${\rm
ELPD}_{\rm approx} =$ `r fmt(loo_cb$estimates[1, 1], 1)`), which
overestimates the predictive performance. Plotting the Pareto $k$ estimates
reveals that the model had to be refit `r nrefits` times, out of a total of $N -
L =$ `r N - L` predicted observations (see Figure \@ref(fig:cb-pareto-k)). On
average, this means one refit every `r fmt((N - L) / nrefits, 1)` observations,
which implies a drastic speed increase as compared to exact LFO-CV.

Performing LFO-CV of 4-SAP, we compute ${\rm ELPD}_{\rm exact} =$ 
`r fmt(sum_cb_elpd_4sap_exact, 1)` and ${\rm ELPD}_{\rm approx} =$ 
`r fmt(sum_cb_elpd_4sap_approx, 1)`, which are again similar but not as close as
the corresponding 1-SAP results. This is to be expected as the uncertainty of
PSIS-LFO-CV increases for increasing $M$ (see Section \@ref(simulations)). As
displayed in the right panel of Figure \@ref(fig:cb-pw-elpd), the pointwise 
ELPD contributions are highly accurate, with no outliers, indicating the our
approximation has worked well consistently across observations. For
constant threshold $\tau$, the importance weights are the same independent of
$M$, so the Pareto $k$ estimates are the same for $4$-SAP and $1$-SAP.

```{r cb-pw-elpd, warning=FALSE, fig.height=3, fig.cap="Pointwise exact vs. PSIS-approximated ELPD contributions of 1-SAP (left) and 4-SAP (right) for the cherry blossom model. A threshold of $\\tau = 0.6$ was used for the Pareto $k$ estimates. $M$ is the number of predicted future observations."}
cb_pw_elpd <- tibble(
  elpd_exact = na.omit(cb_elpd_1sap_exact),
  elpd_approx = na.omit(cb_elpd_1sap_approx),
  k = na.omit(attributes(cb_elpd_1sap_approx)$ks),
  M = "M = 1"
) %>% bind_rows(
  tibble(
    elpd_exact = na.omit(cb_elpd_4sap_exact),
    elpd_approx = na.omit(cb_elpd_4sap_approx),
    k = na.omit(attributes(cb_elpd_4sap_approx)$ks),
    M = "M = 4"
  )
)
ggplot(cb_pw_elpd, aes(elpd_exact, elpd_approx)) +
  facet_wrap(facets = "M", nrow = 1, ncol = 2, scales = "free") +
  geom_abline(slope = 1) +
  geom_point() +
  labs(y = "Approximate ELPD", x = "Exact ELPD") + 
  theme_bw()
```

```{r cb-pareto-k, fig.height=2.5, fig.cap="Pareto $k$ estimates for PSIS-LFO-CV of the cherry blossom model leaving out all future values. The dotted red line indicates the threshold at which the refitting was necessary."}
ks <- na.omit(attributes(cb_elpd_1sap_approx)$ks)
ids <- (L + 1):N
plot_ks(ks, ids)
```


# Conclusion {#discussion}

We proposed, evaluated, and demonstrated PSIS-LFO-CV, a new method for approximating
cross-validation methods for time-series models. PSIS-LFO-CV is intended to  
be used when the prediction task is predicting future values based
solely on past values and thus leave-one-out cross-validation is inappropriate. 
Within the set of such prediction tasks, we can choose
the number $M$ of future values to be predicted at a time.

For a set of common time-series models, we established via simulations that
PSIS-LFO-CV is an almost unbiased approximation of exact LFO-CV if we choose the
threshold $\tau$ of the Pareto $k$ estimates to be not larger than $\tau = 0.6$.
Because the number of required model refits (and thus the computation time)
increases with decreasing $\tau$, we currently see $\tau = 0.6$ as a good
default when performing PSIS-LFO-CV. This is noticeably smaller than the
recommended threshold for PSIS-LOO-CV of $\tau = 0.7$ due to the greater 
dependence in the errors in the LFO case.

Lastly, we want to briefly note that LFO-CV can also be used to compute marginal
likelihoods. Using basic rules of conditional probability, we can factor the
log marginal likelihood as

\begin{equation}
\log p(y) = \sum_{i=1}^N \log p(y_i \,|\, y_{1:i}).
\end{equation}

This is exactly the ELPD of 1-SAP if we set $L = 0$, that is if we
choose to predict *all* observations using their respective past (the very
first observation is only predicted from the prior). As such, marginal
likelihoods may be approximated using PSIS-LFO-CV. Although this approach is
unlikely to be more efficient than methods specialized to compute marginal
likelihoods, such as bridge sampling [@meng1996; @meng2002; @gronau2017], it may
be a noteworthy option if for some reason other methods fail.

# Acknowledgments

We thank Daniel Simpson, Shira Mitchell, and M\r{a}ns Magnusson for helpful 
comments and discussions on earlier versions of this paper.

\newpage

# Appendix {-}

## Appendix A: Pseudo code for PSIS LFO-CV {-}

The R flavored pseudo code below provides a description of the proposed
PSIS-LFO-CV algorithm when leaving out all future values. See https://github.com/paul-buerkner/LFO-CV-paper for the actual R code.

```{r, tidy=FALSE, eval=FALSE, echo = TRUE}
PSIS_LFO_CV = function(model, data, M, L, tau) {
  # Arguments:
  #   model: the fitted time-series model based on the complete data
  #   data: the complete data set
  #   M: number of steps to be predicted into the future
  #   L: minimal number of observations necessary to make predictions
  #   tau: threshold of the Pareto-k-values
  # Returns:
  #   PSIS approximated ELPD value when leaving out all future values
  N = number_of_rows(data)
  S = number_of_draws(model)
  LL_matrix = matrix(nrow = S, ncol = N)
  out = vector(length = N)
  model_star = model
  i_star = N
  LL = log_likelihood(model_star, data = data)
  LL_matrix[, (N - M + 1):N] = LL[, (N - M + 1):N]
  for (i in (N - M):L) {
    LL = log_likelihood(model_star, data = data[1:(i + M), ])
    LL_matrix[, i] = LL[, i]
    PSIS_object = PSIS(-sum_per_row(LL_matrix[, (i+1):i_star]))
    k = pareto_k_values(PSIS_object)
    if (k > tau) {
      # refitting the model is necessary
      i_star = i
      model_star = update(model_star, data = data[1:i, ])
      LL = log_likelihood(model_star, data = data[1:(i + M), ])
      LL_matrix[, i] = LL[, i]
      out[i] = log_mean_exp(sum_per_row(LL[, (i + 1):(i + M)]))
    } else {
      # PSIS approximation is possible
      LW = log_weights(PSIS_object)
      out[i] = log_sum_exp(LW + sum_per_row(LL[, (i + 1):(i + M)]))
    }
  }
  return(sum(out))
}
```

